<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Swetha Adike</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Satisfy"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">


</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex justify-content-center align-items-center header-transparent">

    <nav id="navbar" class="navbar">
      <ul>
        <li class="textList"><a class="nav-link scrollto active" href="#hero">Home</a></li>
        <li class="textList"><a class="nav-link scrollto" href="#about">About</a></li>
        <li class="textList"><a class="nav-link scrollto" href="#resume">Skills</a></li>
        <li class="textList"><a class="nav-link scrollto" href="#experience">Experience</a></li>
        <li class="textList"><a class="nav-link scrollto" href="#education">Education</a></li>

        <!-- <li class="textList"><a class="nav-link scrollto" href="#services">Services</a></li>
        <li class="textList"><a class="nav-link scrollto " href="#portfolio">Portfolio</a></li> -->
        <li class="textList"><a class="nav-link scrollto" href="#contact">Contact</a></li>
        <li class="textList"><a class="nav-link scrollto" href="./assets/resume/Swetha Data Engineer.pdf" target="_blank">Resume</a></li>

      </ul>
      <i class="bi bi-list mobile-nav-toggle"></i>
    </nav><!-- .navbar -->

  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero">
    <div class="hero-container">
      <h1> Swetha Adike</h1>
      <h2>I'm a Senior Data Engineer</h2>
      <a href="#about" class="btn-scroll scrollto" title="Scroll Down"><i class="bx bx-chevron-down"></i></a>
    </div>
  </section><!-- End Hero -->

  <main id="main">

    <!-- ======= About Me Section ======= -->
    <section id="about" class="about">
      <div class="container">

        <div class="section-title">
          <span>About Me</span>
          <h2>About Me</h2>

          <p>Senior Data Engineer with 8+ years of professional IT experience with Big Data experience in Hadoop
            ecosystem components in ingestion, Data modeling, querying, processing, storage, analysis, Data
            Integration and Implementing enterprise level systems spanning Big Data.</p>
        </div>

        <div class="row">
          <div class="image col-lg-4 d-flex align-items-stretch justify-content-center justify-content-lg-start"></div>
          <div class="col-lg-8 d-flex flex-column align-items-stretch">
            <div class="content ps-lg-4 d-flex flex-column justify-content-center">
              <div class="row">
                <div class="col-lg-12">
                 
                    <p> Worked on extensive migration of Hadoop and Spark Clusters to AWS and Azure.
                    </p>
                    <p>Experience in BI Development, Analysis Datacenter Migration, Azure Data Factory (ADF) V2. Managing 
                      Database, Azure Data Platform services (Azure Data Lake (ADLS), Data Factory (ADF), Data Lake 
                      Analytics, Stream Analytics, Azure SQL DW, Azure Synapse, HDInsight/Databricks, NoSQL DB), SQL 
                      Server, Oracle, Data Warehouse etc.</p>
                    <p>Experience in installing, configuring, and using Apache Hadoop ecosystem components like Hadoop 
                      Distributed File System (HDFS), MapReduce, Yarn, Spark, Nifi, Pig, Hive, Flume, Hbase, Oozie, 
                      Zookeeper, Sqoop, Scala</p>
                    <p>Experience with AWS: EC2, VPC, EBS, S3, IAM, Auto Scaling Group, opsWorks, AMI (Amazon Machine 
                      Image), Cloud formation template, CloudTrail, SNS, Route 53, cloud Formation, CloudTrail, CloudWatch
                    </p>
                    <p>Build multiple Data Lakes. Build how the data will be received, validated, transformed and then published.

                    </p>
                    <p>Experience in implementing hybrid connectivity between cloud and on - premises using virtual networks, VPN 
                      and Express Route
                    </p>
                    <p>Extensive experience in working with different ETL tool environments like SSIS, Informatica and reporting 
                      tool environments like SQL Server Reporting Services (SSRS).
                    </p >
                    <p>Expertise in SQL Server Analysis Services (SSAS), SQL Server Reporting Services (SSRS) tools and in 
                      development of T-SQL, Oracle PL/SQL Scripts, Stored Procedures and Triggers for business logic 
                      implementation.
                      
                    </p>
                    <p>Azure Analysis Services (SSAS) and SQL Server Reporting Services (SSRS) and Familiarity with Excel 
                      PowerPivot, Power View, and PowerBI
                      
                    </p>
                   
                  
                
                </div>

              </div>

            </div>

          </div>
        </div>

      </div>
    </section><!-- End About Me Section -->

    <!-- ======= My Resume Section ======= -->
    <section id="resume" class="resume">
      <div class="container">

        <div class="section-title">
          <span>Skills</span>
          <h2>Skills</h2>
          
        </div>

        <div class="row">
          <div class="col-lg-12">
            <ul>
              <li class="textList">
                <strong>Big Data Eco-system </strong>
                HDFS, MapReduce, Spark, Yarn, Hive, Pig, HBase, Sqoop, Flume, Kafka, Oozie, ZooKeeper, Airflow, Impala
              </li>
              <br>
              <li class="textList">
                <strong>Hadoop Technologies</strong>
                Apache Hadoop 1.x, Apache Hadoop 2.x, Cloudera CDH4/CDH5, Hortonworks
              </li>
              <br>

              <li class="textList"><strong>ETL Tools</strong>
                Azure Data Factory (ADF), Azure Database migration Service (DMS), ETL SQL Server 
                Integration Services (SSIS), SQL Server Reporting Services (SSRS), ETL Extract Transformation and Load., 
                Business Intelligence (BI).
              </li>
              <br>

              <li class="textList"><strong>Storage
                </strong>
                Azure Storage, Azure Blob Storage, Azure Backup, Azure Storage Disks- Premium, Azure Files, 
Azure Data Lake Storage Gen 1, AWS S3 Cloud Storage, AWS EBS, AWS Storage gateway, NETApp/EMC 
Storage Technologies, SAN</li>
              <br>
              <li class="textList"><strong>Cloud Computing </strong>
                Microsoft Azure Cloud technologies, Snowflake, Azure Backup AWS (Amazon web 
                Services) Data Modeling logical and physical Data Modeling /Database design using Erwin and Visio etc              </li>
              <br>
              <li class="textList">
                <strong>Databases</strong>
                Azure SQL Database, AZURE data warehouse, Azure Data azure data factory, Azure Data Synch, 
                Elastic Pools, SQL Server 2017, Oracle EBS R12, Oracle EBS 11i, SQL Server 2016, SQL Server 2014, SQL 
                Server 2012,2008 R2, SQL 2005,2000, Azure SQL Database, RDBMS, Azure Synapse, Microsoft Azure VM, 
                Business Intelligence (BI), Amazon Web Service (AWS), AWS Cloud Watch, Oracle11g, MYSQL 5.x, MS 
                Access, OLAP, OLTP, AWS RDS, AWS Cloud, Microsoft Azure Cloud, Cosmos DB, Mongo DB.              </li> <br>
              <li class="textList">
                <strong>Scripting</strong>
                Windows Power Shell, Shell Script, AZURE CLI, Transact SQL, AWS CLI, UNIX, Python, 
                Pyspark, SQL, Hive, Spark
              </li> <br>
              <li class="textList">
                <strong>Machine Learning</strong>
                Regression, Decision Tree, Clustering, Random Forest, Classification, SVM, NLP

              </li> <br>
              <li class="textList">
                <strong>Operating Systems</strong>
                Azure Virtual Machine (VM), Windows Server 2016, Windows Server 2012, Windows 
                2008 R2, /2000 Server(64bit), Windows 2000 Advanced Server, Windows XP, Unix, LINUX. Shell Script, 
                AWS EC2
              </li> <br>
              <li class="textList">
                <strong> Container/Cluster Managers</strong>
                Docker, Kubernetes
              </li> <br>
              <li class="textList">
                <strong> BI Tool</strong>
                 Tableau, Power BI
              </li> <br>
              <li class="textList">
                <strong> Web Development</strong>
                 HTML, XML, CSS

              </li> <br>
              <li class="textList">
                <strong> IDE Tools</strong>
                Eclipse, Jupyter, Anaconda, PyCharm

              </li> <br>
              <li class="textList">
                <strong> Development Methodologies</strong>
                Agile, Waterfal

              </li> <br>
              
            </ul>
          </div>

        </div>
    </section><!-- End My Resume Section -->

    <!-- ======= My Experience Section ======= -->
    <section id="experience" class="resume">
      <div class="container">

        <div class="section-title">
          <span>Experience</span>
          <h2>Experience</h2>
        </div>

        <div class="row">
          <div class="col-lg-6">
           
            <div class="resume-item">
              <h4>Senior Data Engineer - Barclays</h4>
              <h5>Apr 2022 - Present</h5>
              <p><em>Whippany, New Jersey, United States</em></p>
              <p>
              <ul>
                <li class="textList">Provided End to end business intelligence solutions by using Microsoft technologies using Azure Data Lake, 
                  Azure Databricks, Azure Data Factory, Azure SQL data warehouse, and Azure Synapse.</li>
                <li class="textList">Extract Transform and Loaded data from Sources Systems to Azure Data Storage services using a 
                  combination of Azure Data Factory, T-SQL, Spark SQL, and U-SQL Azure Data Lake Analytics. Data 
                  Ingestion to one or more Azure Services - (Azure Data Lake, Azure Storage, Azure SQL, Azure DW) and 
                  processing the data in In Azure Databricks</li>
                <li class="textList">Used Azure Data Factory or Azure Databricks to create ETL workflows that integrate with PostgreSQL
                  instances.
                  </li>
                <li class="textList">Optimized SQL queries that retrieve, insert, update, or delete data in PostgreSQL databases, using Azure 
                  Data Studio to write and debug SQL queries.
                  </li>
                  <li class="textList"> The idea behind Data Mesh is to treat data as a product, with each domain or business unit responsible for its 
                    own data products, rather than relying on a central team to manage all data for the organization. This approach 
                    aims to improve data quality, ownership, and accountability, as well as to enable better collaboration between 
                    data producers and consumers</li>
                  <li class="textList">Created CI/CD Pipelines in ADF using Linked Services/Datasets/Pipeline/ to ETL data from different 
                    sources like Azure SQL, Blob storage, Azure SQL Data warehouse, write-back tool and backwards.</li>
                  <li class="textList">Integrated snowflake with a variety of Azure and AWS services, such as Azure Data Factory or AWS Glue, 
                    to enable data ingestion and processing.</li>
                    <li class="textList">Used snowflake for data validation, data profiling, and data lineage to ensure the quality of the data.</li>
                    <li class="textList">Developed application code that interacts with the PostgreSQL database and used a PostgreSQL driver to 
                      connect to the database and execute SQL queries.
                      </li>
                      <li class="textList">
                        Developed Azure Synapse Notebooks using Pyspark and Spark-SQL for data extraction, transformation, and 
aggregation from multiple file formats for analyzing & transforming the data to uncover insights into the 
customer usage patterns
                      </li>
                      <li class="textList">Used Oracle Database in Azure to run mission-critical Oracle workloads, migrate Oracle databases to 
                        Azure, and integrate with other Azure services like Azure Active Directory, Azure Monitor, and Azure 
                        Backup.</li>
                        <li class="textList">
                          Created scripts as per snowflake supported datatypes and worked on snowflake stored procedures 
framework
                        </li>
                        <li class="textList">
                          Used AWS services as a back end for Azure application, such as using AWS S3 for storage or AWS Lambda
for serverless computing.

                        </li>
                        <li class="textList">
                          Developed ETL Processes in AWS Glue to migrate data from external sources like S3, ORC/Parquet/Text 
Files into AWS Redshift
                        </li>
                        <li class="textList">
                          Worked on Ingesting data by going through cleansing and transformations and leveraging AWS Lambda, AWS 
                          Glue and Step Functions
                        </li>
                        <li class="textList">
                          <strong>
                            Environment : 
                          </strong>
                          Azure Data Factory, Azure Synapse, Azure Databricks, Azure Lake Gen2, Azure SQL database,
                          AWS, Principal component Analysis (PCA), Snowflake, PostgreSQL, Hadoop, Oracle, Map Reduce, Yarn, Hive, 
                          Python, Spark, Scala, SSMS, Horton Works, Data Lake, Databricks, Power BI Desktop, Power BI query, DAX.
                          
                        </li>

              </ul>
              </p>
            </div>
            <div class="resume-item">
              <h4>Senior Data Engineer - Northwell Health</h4>
              <h5>Feb 2021 - Apr 2022 </h5>
              <p><em>New York , United States</em></p>
              <p>
              <ul>
                <li class="textList">Involved in Requirement gathering, Business Analysis and translated business requirements into technical
                  design in Hadoop and Big Data.</li>
                <li class="textList">Involved in SQOOP implementation which helps in loading data from various RDBMS sources 
                  to Hadoop systems.</li>
                <li class="textList">Developed Python scripts to extract the data from the web server output files to load into HDFS.
 </li>
                <li class="textList">Involved in HBASE setup and storing data into HBASE, which will be used for further analysis.</li>
                  <li class="textList">
                    Worked on Written a python script which automates to launch the EMR cluster and configures the Hadoop
applications.

                  </li>
                  <li class="textList">
                    Created various data pipelines using PySpark, Scala and SparkSQL for faster processing of data.
                  </li>
                  <li class="textList">
                    Written Spark-SQL and embedded the SQL in SCALA files to generate jar files for submission onto the 
                    Hadoop cluster.
                  </li>
                  <li class="textList">
                    Worked with Avro and Parquet, ORC, XML, JSON files and converted the data from either format Parsed
Semi Structured JSON data or converted to Parquet using Data Frames in PySpark
                  </li>
                  <li class="textList">
                    Developed a Python Script to load the CSV files into the S3 buckets and created AWS S3 buckets, performed 
                    folder management in each bucket, managed logs, and objects within each bucket.
                    
                  </li>
                  <li class="textList">
                    Involved in Analyzing system failures, identifying root causes, and recommended course of actions, 
                    Documented the systems processes and procedures for future references.
                    
                  </li>
                  <li class="textList">
                    Experience in bulk loading and continuous ingestion of data using COPY and Snow pipe respectively.

                  </li>
                  <li class="textList">
                    Involved in Hadoop installation, Commissioning, Decommissioning, Balancing, Troubleshooting, Monitoring 
and, debugging Configuration of multiple nodes using Hortonworks platform
                  </li>
                  <li class="textList">
                    Involved in working with Spark on top of Yarn for interactive and Batch Analysis
                  </li>
                  <li class="textList">
                    Worked with a team to migrate from Legacy/On prem environment into AWS.
                  </li>
                  <li class="textList">
                    Handled data modeling in snowflake and Integrated Snowflake with AWS Lambda and Step functions.
                  </li>
                  <li class="textList">
                    Writing stored procedures in snowflake using JavaScript/SQL.
                  </li>
                  <li class="textList">
                    Created Dockized backend cloud applications with exposed Application Program Interface (API) interfaces 
                    and deployed on Kubernetes.
                    
                  </li>
                  <li class="textList">
                    Experienced in analyzing and Optimizing RDD's by controlling partitions for the given data
                  </li>
                  <li class="textList">
                    <strong>
                      Environment
                    </strong>
                    HDFS, Hive, Scala, Sqoop, Spark, Tableau, Yarn, Cloudera, SQL, Terraform, Splunk, RDBMS, 
Elastic search, Kerberos, Jira, Confluence, Shell/Perl Scripting, Zookeeper, AWS (EC2, S3, EMR, Redshift, ECS, 
Glue, S3, VPC, RDS etc.), Snowflake, Snow pipe, Informatica, Git, Kafka, CI/CD(Jenkins), Agile, Azure 
Databricks, Tableau, Java.

                  </li>
              
              </ul>
              </p>
            </div>
          </div>
          <div class="col-lg-6">
           
            <div class="resume-item">
              <h4>Data Engineer - Northwell Health</h4>
              <h5>May 2019 – Feb 2021
              </h5>
              <p><em> New York, United States</em></p>
              <p>
              <ul>
                <li class="textList">Writing Scope script to read and load the data into Cosmos portal.</li>
                <li class="textList">Extensive working experience in data warehousing and Business Intelligence Technologies with expertise 
                  in SQL Server development, MSBI stack (TSQL, SSIS, SSAS, SSRS), Azure, Power BI for building,
                  deploying, and managing applications and services through a global network of Microsoft - managed data 
                  centers
                   </li>
                <li class="textList">Worked with business stakeholders and data analysts to define the master data model (MDM) for the project, 
                  involves identifying the key data entities and attributes, and defining relationships and hierarchies between 
                  them.</li>
                <li class="textList">Analyze, design, and build Modern data solutions using Azure PaaS service to support visualization of data. 
                  Understand current Production state of application and determine the impact of new implementation on existing 
                  business processes.</li>
                  <li class="textList">
                    Extract Transform and Load data from Sources Systems to Azure Data Storage services using a combination of 
Azure Data Factory, T-SQL, Spark SQL, and U-SQL Azure Data Lake Analytics. Data Ingestion to one 
or more Azure Services - (Azure Data Lake, Azure Storage, Azure Synapse, Azure SQL, Azure DW) and 
processing the data in Azure Databricks
                  </li>
                  <li class="textList">
                    Created Pipelines in ADF using Linked Services/Datasets/Pipeline/ to Extract, Transform, and load data
                    from different sources like Azure SQL, Blob storage, Azure SQL Data warehouse, write-back tool and 
                    backwards
                    </li>
                    <li class="textList">
                      Developed Spark applications using Pyspark and Spark-SQL for data extraction, transformation, and 
aggregation from multiple file formats for analyzing & transforming the data to uncover insights into the 
customer usage patterns.

                    </li>
                    <li class="textList">
                      Responsible for estimating the cluster size, monitoring, and troubleshooting of the Spark data bricks cluster.

                    </li>
                    <li class="textList">
                      Experienced in performance tuning of Spark Applications for setting right Batch Interval time, correct level 
                      of Parallelism and memory tuning
                    </li>
                    <li class="textList">
                      To meet specific business requirements wrote UDF’s in Scala and Pyspark
                    </li>
                    <li class="textList">
                      Developed JSON Scripts for deploying the Pipeline in Azure Data Factory (ADF) that process the data using 
the SQL Activity
                    </li>
                    <li class="textList">
                      Architected Cubes with PowerPivot, MDX, SSAS.
                    </li>
                    <li class="textList">
                      Hands-on experience on developing SQL Scripts for automation purpose.

                    </li>
                    <li class="textList">
                      Created POWER BI Visualizations and Dashboards as per the requirements
                    </li>
                    <li class="textList">
                      SQL Server Analysis Services (SSAS) and SQL Server Reporting Services (SSRS) and Familiarity with 
Excel PowerPivot, Power View, and PowerBI.
                    </li>
                    <li class="textList">
                      <strong>
                        Environment
                      </strong>
                      Azure Data Lake, Azure Storage, Azure Synapse, Azure SQL, Azure DW, Pyspark, SQL, MDM, 
ADF, T-SQL, Power BI Desktop, Power Bi Pro, SQL, SSIS, SSRS.

                    </li>
              </ul>
              </p>
            </div>
            <div class="resume-item">
              <h4>Data Engineer - ROKT</h4>
              <h5>May 2017 - Feb 2019</h5>
              <p><em>New York, United States</em></p>
              <p>
              <ul>
                <li class="textList">Analyzed and reported customer using data transactional and analytical data to meet business objectives</li>
                <li class="textList">Worked on the entire CRISP-DM life cycle and actively involved in all the phases of project life cycle including 
                  data acquisition, data cleaning, data engineering
                  </li>
                <li class="textList">Developed weekly, monthly reports related to the marketing and financial departments using Teradata SQL.</li>
                <li class="textList">Designed high level ETL architecture for overall data transfer from the OLTP to OLAP with the help of SSIS.
                </li>
                <li class="textList">
                  Extracted data from SQL Server using Talend to load it into a single data warehouse repository. Designed 
tabular, matrix reports, drilldown, drill through, Parameterized and linked reports in SSRS.
                </li>
                <li class="textList">
                  Wrote SQL queries using joins, grouping, nested sub-queries, and aggregation depending on data needed 
from various relational customer databases.

                </li>
                <li class="textList">
                  Created V-Look Up functions in MS Excel for searching data in large spreadsheets.

                </li>
                <li class="textList">
                  Worked with Oracle Data stage for ETL processes.

                </li>
                <li class="textList">
                  Developed ad-hoc reports with V-lookups, Pivot tables, and Macros in Excel and recommended solutions to 
drive business decision making
                </li>
                <li class="textList">
                  Designed data models and data flow diagrams using Erwin and MSVisio.
                  </li>
                  <li class="textList">
                    Involved in Trouble Shooting, Performance tuning of reports, resolving issues within Tableau Server & 
Reports
                  </li>
                  <li class="textList">
                    <strong>Environment</strong>
                    Crystal Reports, DB2 SQL, SQL Server 2008, SQL, Tableau, SSIS, SSRS.

                  </li>

              </ul>
              </p>
            </div>
            <div class="resume-item">
              <h4>Big Data & Hadoop Developer - Synchrony Technologies</h4>
              <h5>Sep 2013 - Jul 2016 </h5>
              <p><em>Hyderabad, India</em></p>
              <p>
              <ul>
                <li class="textList">Involved in designing, reviewing, implementing, testing, and releasing databases, sign specifications, and 
                  application software.
                  </li>
                <li class="textList">Responsible for installation, gradation of different environments, creating and maintaining oracle development 
                  and test databases.
                  </li>
                <li class="textList">Wrote and modified Oracle PL/SQL, SQL statements procedures, and functions.</li>
                <li class="textList">Developed a data model for the payout stream using Erwin as a data modeling tool.
                </li>
                <li class="textList">
                  Responsible for users, roles, and privilege management.

                </li>
                <li class="textList">
                  Created PL/SQL sub setting scripts to mask the PHI columns across various IT environments as part of TDM Database 
                  Refresh Process occurring every quarter based on the business
                </li>
                <li class="textList">
                  Migrated data from production (AIG) to test the deferred products with the development engine
                </li>
                <li class="textList">
                  Responsible for the definition and management of schemas and objects.
                </li>
                <li class="textList">
                  Supporting and advising developers during databases related issues.

                </li>
                <li class="textList">
                  Rewrite SQL queries to improve the performance of the application.

                </li>
              </ul>
              </p>
            </div>
          </div>
        </div>

      </div>
    </section><!-- End My Resume Section -->

    <!-- ======= My Education Section ======= -->
    <section id="education" class="resume">
      <div class="container">

        <div class="section-title">
          <span>Education</span>
          <h2>Education</h2>
        </div>

        <div class="row">
          <div class="col-lg-6">
            <!-- <h3 class="resume-title">Education</h3> -->
            <div class="resume-item">
              <h4>Masters in Information Systems (Major: Business Analytics)</h4>
              <h5> Aug 2016 - Dec 2017</h5>
              <p><em>Marist College, Poughkeepsie, NY</em></p>
              <p> GPA : 3.98</p>
            </div>


          </div>
          <div class="col-lg-6">
            <!-- <h3 class="resume-title">Education</h3> -->

            <div class="resume-item">
              <h4>Bachelor of Technology in Civil Engineering</h4>
              <h5>Aug 2009- May 2013</h5>
              <p><em>Gokaraju Rangaraju Institute of Engineering and Technology, India</em></p>
              <p>GPA : 3.81</p>
            </div>
          </div>
        </div>

      </div>
    </section><!-- End My Resume Section -->

    <!-- ======= Contact Me Section ======= -->
    <section id="contact" class="contact">
      <div class="container">

        <div class="section-title">
          <span>Contact Me</span>
          <h2>Contact Me</h2>
        </div>

        <div class="row">

          <div class="col-lg-6">

            <div class="row">
              <div class="col-md-6">
                <div class="info-box mt-4">
                  <i class="bx bx-envelope"></i>
                  <h3>Email Me</h3>
                  <p>

                    <a href="mailto:swethaad26@gmail.com" class="linkin-class"
                      target="_blank">swethaad26@gmail.com</a>
                  </p>
                </div>
              </div>
              <div class="col-md-6">
                <div class="info-box mt-4">
                  <i class="bx bxl-linkedin"></i>
                  <h3>Linked In</h3>
                  <p><a class="linkin-class"
                      href="https://www.linkedin.com/in/swetha-ad/"
                      target="_blank">www.linkedin.com/in/swetha-ad/
                    </a></p>
                </div>
              </div>

            </div>

          </div>

          <div class="col-lg-6">
            <div class="row">
              <div class="col-md-6">
                <div class="info-box mt-4">
                  <i class="bx bx-phone-call"></i>
                  <h3>Call Me</h3>
                  <p> 848-216-5340</p>
                </div>
              </div>
              <div class="col-md-6">
                <div class="info-box mt-4">
                  <i class="bx bx-map"></i>
                  <h3>Location</h3>
                  <p>Franklin Park, New Jersey</p>
                </div>
              </div>

            </div>
          </div>

        </div>

      </div>
    </section><!-- End Contact Me Section -->

  </main><!-- End #main -->



  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i
      class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>